# 3D-R2N2 代码整理

## 1. LSTM和GRU

### 1.1 LSTM

> LSTM单元是RNN隐藏状态最成功的实现之一。LSTM单元明确控制从输入到输出的流量，允许网络克服消失的梯度问题。
>
> 具体来说，LSTM单元由四个部分组成：

+ 存储单元（存储单元和隐藏状态)
+ 控制从输入到隐藏状态（输入门）
+ 从隐藏状态到输出的信息流的三个门 （输出门）
+ 从前一个隐藏状态到当前隐藏状态（忘记门）。
在时间t时新输入𝑥_𝑡

#### 1.2 GRU

> GRU是LSTM的一个变种
>
> GRU的一个优点是与标准LSTM相比计算量更少.在GRU中，更新门控制输入和忘记门
> 另一个区别是在非线性变换之前应用复位门。

## 2. 重构过程

> 采用深度学习从2D图像到其对应的3D voxel模型的映射: 首先利用一个标准的CNN结构对原始input image 进行编码； 再利用一个标准 Deconvolution network 对其解码。 中间用Long Short-Term Memory（LSTM是RNN中的一种）进行过渡连接, LSTM 单元排列成3D网格结构, 每个单元接收一个feature vector from Encoder and Hidden states of neighbors by convolution，并将他们输送到Decoder中. 这样每个LSTM单元重构output voxel的一部分。 总之，通过这样的Encoder-3DLSTM-Decoder 的网络结构就建立了2D images -to -3D voxel model 的映射.
![Overview](imgs/full_network.png)

## 3. LSTM的优点

使用基于LSTM的网络的主要优点来自于当多个视图被馈送到网络时有效处理对象自遮挡的能力。 网络选择性地更新对应于对象的可见部分的存储器单元。 如果后续视图显示先前自遮挡且与预测不匹配的部分，则网络将更新先前被遮挡的部分的LSTM状态，但保留其他部分的状态。

网络的目标是执行单视图和多视图3D重建。主要思想是利用LSTM的功能来保留先前的观测结果，并随着更多的观测结果逐步完善输出重建。

## 4. 网络架构

> 网络架构：每个3D-R2N2由编码器（a 2D Convolutional Neural Network (2D-CNN)），递归单元（3D-LSTM）和解码器（ 3D Deconvolutional Neural Network (3D-DCNN)）组成。 在每个卷积层之后，我们放置LeakyReLU层用于非线性化。 编码器将127×127大小的RGB图像转换为低维特征，然后将其馈送到3D-LSTM。 然后解码器获取3D-LSTM隐藏状态并将它们转换为最终体素占用图。 在每个卷积层之后是LeakyReLU层。我们使用两个版本的3D-R2N2 :(顶部）浅网络和（底部）深度残留网络

### 4.1 编码器2D-CNN

使用CNN将2D图片转换为特征
我们设计了两种不同的2D-CNN编码器

+ 标准前馈CNN
+ 深度残差版本CNN。

> 第一个网络由标准卷积层，汇集层和leakyRELU线性单元组成，后跟Full-connected层。我们还创建了第一个网络的深度残差版本。
>
> 在标准卷积层之间添加**残差连接**有效地改善并加速了深层网络的优化过程。除了第4对之外，编码器网络的深度残差版本在每2个卷积层之后具有特征映射连接。为了匹配卷积后的通道数，我们使用1×1卷积进行残余连接。然后将编码器输出展平并传递到完全连接的层，该层将输出压缩成1024维特征向量。

### 4.2 循环：3D-Convolutional LSTM

我们的3D-R2N2的核心部分是一个recurrence模块，它允许网络保留它看到的内容并在看到新图像时更新对应单元的内容。
一种简单的方法是使用现成的LSTM网络。然而，在没有任何正规化的情况下，预测如此大的输出空间（32×32×32）将是一个非常困难的任务。

3D-R2N2提出一种新的网络架构，称这种新的网络架构为3D-Convolutional-LSTM（3D-LSTM），该网络由一组具有受限连接的结构化LSTM单元组成。3D-LSTM单元在空间上以3D网格结构排布，每个单元负责最终输出的一部分重建工作。在3D网格内部，有N\ \times N\ \times N个3D-LSTM单元，其中N是3D-LSTM网格的空间分辨率。每个3D-LSTM单元具有索引\left(i,j,k\right)，每个3D-LSTM单元具有独立的隐藏状态h_t。

空间结构化的3D卷积LSTM只允许其隐藏状态ℎ𝑡（𝑖，𝑗，𝑘）受到其相邻3D-LSTM单元的影响。更具体地，相邻连接由卷积内核大小定义。例如，如果我们使用3×3×3内核，则LSTM单元仅受其直接邻居的影响。这样，单元可以共享权重，并且网络可以进一步正则化。

### 4.3 解码器Decoder

在接收到输入图像序列𝑥1，𝑥2，...，𝑥𝑇之后，3D-LSTM将隐藏状态p传递给解码器，通过应用3D解卷积，非线性和3D解池化来增加隐藏状态分辨率，直到它达到目标输出分辨率。

与编码器一样，我们提出了一个简单的解码器网络，其中包含5个卷积。
还有一个深度残差版本，其中包含4个残余连接加上最终的卷积层。 在激活达到目标输出分辨率的最后一层之后，我们使用体素形式的softmax函数将最终的activation V转换为在(i,j,k)位置体素存在的概率p(i,j,k)。

### 4.4 丢失层：3D 体素形式的softmax

网络的损失函数被定义为体素交叉熵的总和。 让每个体素（i，j，k）的最终输出为伯努利分布

```math
[1 - p_{(i, j, k)}, p_{(i, j, k)}]
```

其中对输入的依赖

```math
{X} = \{x_t\}_{t \in \{1,\ldots ,T\}}
```

被省略，并且相应的空间占有率为

```math
y_{(i, j, k)} \in \{0, 1\}
```

则

```math
\begin{aligned} L(\mathcal {X}, y)&= \sum _{i,j,k} y_{(i, j, k)} \log (p_{(i, j, k)}) + (1 - y_{(i, j, k)}) \log (1 - p_{(i, j, k)}) \end{aligned}
```

## 5. 实现过程

### 5.1数据增强

在训练中，我们使用3D CAD模型生成输入图像和实况体素占用图。我们首先使用透明背景渲染CAD模型，然后使用PASCAL VOC 2012数据集中的随机物体增强输入图像。此外，我们着色模型的颜色并随机翻译图像。 请注意，所有视点都是随机采样的。

### 5.2 训练

在训练网络时，我们使用不定长度输入，任选从一个图像到任意数量的图像。
单个批次中的每个训练样本的输入长度（视图的数量）保持不变，但是不同小批量的训练样本的输入长度随机变化。
这使网络能够执行单视图和多视图重建。
在训练期间，我们仅在输入序列的末尾计算损耗，以便节省计算能力和存储器。另一方面，在测试时间期间，我们可以通过提取LSTM单元的隐藏状态来访问每个时间步的中间重建。

### 5.3 网络

输入图像大小设置为127×127。输出体素化重建的大小为32×32×32。实验中使用的网络经过60,000次迭代的训练，批量大小为36，除了[Res3D-GRU-3]（参见表1），其需要批量大小为24以适合NVIDIA Titan X GPU。
对于LeakyReLU层，leak斜率在整个网络中设置为0.1。对于反卷积，我们遵循上面中提出的解放方案。我们使用Theano来实现我们的网络，并使用Adam来实现SGD更新规则。
我们使用MSRA初始化除LSTM权重之外的所有权重，并且对所有偏差初始化0.1。对于LSTM权重，我们使用SVD来分解随机矩阵并使用归一化矩阵进行初始化。
奇异值分解（SGD,singular value decomposition）

[SGD](https://en.wikipedia.org/wiki/SDG)

## 6. 代码阅读

### 6.1 重建进程函数

> lib.data_process.py.ReconstructionDataProcess.run line113

### 6.2 文件路径

> './ShapeNet/ShapeNetRendering/%category/%model_name/rendering/%2d.png'
> './ShapeNet/ShapeNetVox32/%category/%model_name/model.binvox'

```py
batch_img = np.zeros( # [n_views, batch_size, channel=3, img_h, img_w]
                (curr_n_views, self.batch_size, 3, img_h, img_w), dtype=theano.config.floatX)
batch_voxel = np.zeros( # [batch_size, n_vox=32, 2, n_vox=32, n_vox=32]
                (self.batch_size, n_vox, 2, n_vox, n_vox), dtype=theano.config.floatX)
```

> 具体而言，万能逼近定理表明：只要有足够的隐藏节点，具有线性输出层和至少一个具有任何“压缩”激活函数（如，logistic sigmoid）的隐藏层的前馈网络可以从一个有限维空间到另一个有限维空间有任意的非零误差逼近任何波莱尔可测函数。
> --第198页，Deep Learning，2016年。
> 最后，以下是用于描述神经网络形状和能力的一些术语：

+ 尺寸：模型中的节点数。
+ 宽度：特定层中的节点数。
+ 深度：神经网络中的层数。
+ 能力：可以通过网络配置学到的函数的类型或结构。有时被称为“ 表征能力 ”。
+ 架构：网络中层和节点的具体排列。

为尽可能避免训练时出现“过拟合”现象，保证足够高的网络性能和泛化能力，确定隐层节点数的最基本原则是：在满足精度要求的前提下取尽可能紧凑的结构，即取尽可能少的隐层节点数。研究表明，隐层节点数不仅与输入/输出层的节点数有关，更与需解决的问题的复杂程度和转换函数的型式以及样本数据的特性等因素有关。
在确定隐层节点数时必须满足下列条件：
（1）隐层节点数必须小于N-1（其中N为训练样本数），否则，网络模型的系统误差与训练样本的特性无关而趋于零，即建立的网络模型没有泛化能力，也没有任何实用价值。同理可推得：输入层的节点数（变量数）必须小于N-1。
(2) 训练样本数必须多于网络模型的连接权数，一般为2~10倍，否则，样本必须分成几部分并采用“轮流训练”的方法才可能得到可靠的神经网络模型。 
总之，若隐层节点数太少，网络可能根本不能训练或网络性能很差；若隐层节点数太多，虽然可使网络的系统误差减小，但一方面使网络训练时间延长，另一方面，训练容易陷入局部极小点而得不到最优点，也是训练时出现“过拟合”的内在原因。因此，合理隐层节点数应在综合考虑网络结构复杂程度和误差大小的情况下用节点删除法和扩张法确定。

**在人工神经网络中，仅仅当数据必须被非线性分离的时候，隐藏层才是必需的.**

[人工神经网络中究竟使用多少隐藏层和神经元](https://segmentfault.com/a/1190000015920173)

### 6.3 文件信息整理

main.py是主运行文件，读取命令行参数，运行训练或者测试 由--test参数决定

train_net.py里面先进行网络的初始化，使用一个solver来封装整个net，然后创建数据读取进程队列
包括两组各15个进程的进程队列

对数据的处理是先读取json文件 json文件里面包含了10种要训练的类以及他们的id和类名

将category和name做成元祖，封装成list返回

def get_next_minibatch(self): data_process.py line 62

num_data指的所有的category_name的数目

+ 将category_name的顺序打乱，每次读取batch_size个cate-name
+ 在0-cfg.TRAIN.NUM_RENDERING之间取n_view个数作为图片的下标
+ 读取n_view张图片 + 1个voxel矩阵

> x.shape # 图片
> (curr_n_views, self.batch_size, 3, img_h, img_w)
> y.shape # 体素矩阵
> (self.batch_size, 2, n_vox, n_vox, n_vox)

batch_voxel[batch_id, 0, :, :, :] = voxel_data < 1
batch_voxel[batch_id, 1, :, :, :] = voxel_data
基本就是取反 一个是true一个是false

### 6.4 loss函数

求的是分类问题的交叉熵NLL

```math
L(\mathcal {X}, y) = \sum _{i,j,k} y_{(i, j, k)} \log (p_{(i, j, k)}) + (1 - y_{(i, j, k)}) \log (1 - p_{(i, j, k)})
```

## 7. 深度残差网络

网络的深度为什么重要？
>因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。

为什么不能简单地增加网络层数？
> 对于原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。
>
>对于该问题的解决方法是正则化初始化和中间的正则化层（Batch Normalization），这样的话可以训练几十层的网络。
>
>虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。
>退化问题说明了深度网络不能很简单地被很好地优化。
>作者通过实验：通过浅层网络+ y=x 等同映射构造深层模型，结果深层模型并没有比浅层网络有等同或更低的错误率，推断退化问题可能是因为深层的网络并不是那么好训练，也就是求解器很难去利用多层网络拟合同等函数。

## 8. 网络结构分析

```py
input_size = (batch, 3, 127, 127)
n_convfilter = [96, 128, 256, 256, 256, 256]

1. input   -------->  conv1a(96*127*127)
2. conv1a  -------->  bn1a(96*127*127)
3. bn1a    -------->  conv1b(96*127*127)
4. conv1b  -------->  bn1b(96*127*127)

5. bn1b
```

## 10. 想到的优化

1. loss函数的优化---训练速度的加快，训练效果的变化？
2. BN的优化 -------涉及训练速度的加快
《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》
~~3. dropout层-----过拟合~~
~~BN层本身有解决部分过拟合~~
3. gan   ---------新的网络 训练难度很大
~~5. 梯度下降算法的区别---adam算法要不要尝试~~ adam算法已经被选用
~~adam学习率的优化~~
~~sgd测试~~
~~4. lstm的优化-------难度很大~~
~~[](https://zhuanlan.zhihu.com/p/34203833)~~
4. 残差结构的优化-----可以考虑
[](https://www.jianshu.com/p/e58437f39f65)
目前简单的conv+bn
可以考虑更简单的直接bn

目前尝试的方法

1. gan ---------
测试结果不满意，长时间的停留0.7
loss函数下降不明显
每2000次一存
__C.TRAIN.SAVE_FREQ = 2000  # weights will be overwritten every save_freq
2. BN ^^^^^^^^^^^采用
确实有加速训练的效果
上面结果是使用 tanh 作为激励函数的结果, 可以看出, 不好的初始化, 让输入数据在激活前分散得非常离散, 而有了 BN, 数据都被收拢了. 收拢的数据再放入激励函数就能很好地利用激励函数的非线性. 而且可以看出没有 BN 的数据让激活后的结果都分布在 tanh 的两端, 而这两端的梯度又非常的小, 是的后面的误差都不能往前传, 导致神经网络死掉了.
没有使用BN的时候，训练速度慢很多
3. loss函数正则项的修改 ---------
可以尝试loss函数的 正则化项的选择
正则项改变以后，直接就exceed limit了

未尝试的方法

1. 残差结构的修改
~~可以把conv+bn 改为 bn~~
不能直接使用bn,需要channel数目的变换

## 11.config.py修改

__C.TRAIN.NUM_ITERATION = 6000  # maximum number of training iterations
__C.TRAIN.VALIDATION_FREQ = 200
__C.TRAIN.NAN_CHECK_FREQ = 200
__C.TRAIN.LEARNING_RATES = {'2000': 1e-5, '6000': 1e-6}
__C.TRAIN.SAVE_FREQ = 1000  # weights will be overwritten every save_freq

--iter 2000 \ 训练次数修改

+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  Off  | 00000000:02:00.0  On |                  N/A |
| 49%   77C    P2   246W / 250W |  11139MiB / 11171MiB |    100%      Default |
+-------------------------------+----------------------+----------------------+

## 12. 底稿修改意见

1. 压缩第二章
2. 增加第二章开头3D-R2N2的过渡
3. ~~表格去’格‘~~
4. 顺序引用

## 13. 5-9号修改意见

1.重点1 ~~残差自己结构的拓展~~
2.重点2 ~~压缩第二章 删减公式~~
3.重点3 ~~实验对比图重新画~~
4.重点4 ~~输入图像~~

## 14. todo

~~添加所有test的程序~~

## 15. 论文

Single Image 3D Interpreter Network  --matlab
Surfnet: Generating 3d shape surfaces using deep residual networks --matlab